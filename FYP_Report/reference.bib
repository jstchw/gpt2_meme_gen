 @misc{radford_wu_child_luan_amodei_sutskever_2019,
    title={Language models are unsupervised multitask learners}, 
    journal={OpenAI}, 
    author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya}, 
    year={2019},
    note={\\Accessed Jan 6 2022 [Online]}
} 

@misc{attention_is_all_you_need,
    title={Attention Is All You Need},
    author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan and Kaiser, Lukasz and Polosukhin, Illia},
    year={2017},
    note={\\Accessed Jan 7 2022 [Online]}
}

@inproceedings{inproceedings,
author = {Harvey, I and Husbands, Phil and Cliff, Dave},
year = {1994},
month = {01},
pages = {392-401},
title = {Seeing the Light: Artificial Evolution, Real Vision},
note = {\\Accessed Jan 7 2022 [Online]}
}

@misc{LRA_benchmark,
author = {Tay, Yi and Denghani, Mostafa and Abnar, Samira and Shen, Yikang and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu and Ruder, Sebastian and Metzler, Donald},
year = {2020},
month = {November},
title = {Long Range Arena: A Benchmark For Efficient Transformers},
note = {\\Accessed Jan 7 2022 [Online]}
}

@misc{ListOps,
author = {Nangia, Nikita and Bowman, Samuel},
year = {2018},
month = {April},
title = {ListOps: A Diagnostic Dataset for Latent Tree Learning},
note = {\\Accessed Jan 7 2022 [Online]}
}

@misc{dank_learning,
author = {Abel L. Peirson and E. Meltem Tolunay},
year = {2018},
month = {June},
title = {Dank Learning: Generating Memes Using Deep Neural Networks},
note = {\\Accessed Jan 7 2022 [Online]}
}

@misc{tesseract_article,
author = {Anthony Kay},
year = {2007},
month = {July},
title = {Tesseract: an Open-Source Optical Character Recognition Engine},
note = {\\Accessed Jan 8 2022 [Online]}
}

@misc{pillow_about,
author = {Alex Clark and Contributors},
year = {2022},
title = {From Pillow Documentation},
note = {\\Accessed Jan 8 2022 [Online]}
}

@misc{eel_github,
author = {Chris Knott},
year = {2021},
title = {Eel GitHub},
note = {\\Accessed Jan 10 2022 [Online]}
}

@article{VALUEVA2020232,
title = {Application of the residue number system to reduce hardware costs of the convolutional neural network implementation},
journal = {Mathematics and Computers in Simulation},
volume = {177},
pages = {232-243},
year = {2020},
issn = {0378-4754},
doi = {https://doi.org/10.1016/j.matcom.2020.04.031},
url = {https://www.sciencedirect.com/science/article/pii/S0378475420301580},
author = {M.V. Valueva and N.N. Nagornov and P.A. Lyakhov and G.V. Valuev and N.I. Chervyakov},
keywords = {Image processing, Convolutional neural networks, Residue number system, Quantization noise, Field-programmable gate array (FPGA).},
abstract = {Convolutional neural networks are a promising tool for solving the problem of pattern recognition. Most well-known convolutional neural networks implementations require a significant amount of memory to store weights in the process of learning and working. We propose a convolutional neural network architecture in which the neural network is divided into hardware and software parts to increase performance and reduce the cost of implementation resources. We also propose to use the residue number system (RNS) in the hardware part to implement the convolutional layer of the neural network. Software simulations using Matlab 2018b showed that convolutional neural network with a minimum number of layers can be quickly and successfully trained. The hardware implementation of the convolution layer shows that the use of RNS allows to reduce the hardware costs on 7.86%–37.78% compared to the two’s complement implementation. The use of the proposed heterogeneous implementation reduces the average time of image recognition by 41.17%.}
}

@misc{neocognitron,
title = {Neocognitron: A Self-organizing Neural Network Model
for a Mechanism of Pattern Recognition
Unaffected by Shift in Position},
author = {Kunihiko Fukushima},
year = {1980},
journal = {Biological Cybernetics},
note = {\\Accessed Jan 11 2022 [Online]}
}

@misc{image_classification_article,
title = {Multi-column Deep Neural Networks for Image Classification},
author = {Dan Ciresan and Ueli Meier and Jurgen Schmidhuber},
year = {2012},
note = {\\Accessed Jan 11 2022 [Online]}
}
