 @misc{radford_wu_child_luan_amodei_sutskever_2019,
    title={Language models are unsupervised multitask learners}, 
    journal={OpenAI}, 
    author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya}, 
    year={2019},
    note={\\Accessed Jan 6 2022 [Online]}
} 

@misc{attention_is_all_you_need,
    title={Attention Is All You Need},
    author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan and Kaiser, Lukasz and Polosukhin, Illia},
    year={2017},
    note={\\Accessed Jan 7 2022 [Online]}
}

@inproceedings{inproceedings,
author = {Harvey, I and Husbands, Phil and Cliff, Dave},
year = {1994},
month = {01},
pages = {392-401},
title = {Seeing the Light: Artificial Evolution, Real Vision},
note = {\\Accessed Jan 7 2022 [Online]}
}

@misc{LRA_benchmark,
author = {Tay, Yi and Denghani, Mostafa and Abnar, Samira and Shen, Yikang and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu and Ruder, Sebastian and Metzler, Donald},
year = {2020},
month = {November},
title = {Long Range Arena: A Benchmark For Efficient Transformers},
note = {\\Accessed Jan 7 2022 [Online]}
}

@misc{ListOps,
author = {Nangia, Nikita and Bowman, Samuel},
year = {2018},
month = {April},
title = {ListOps: A Diagnostic Dataset for Latent Tree Learning},
note = {\\Accessed Jan 7 2022 [Online]}
}

@misc{dank_learning,
author = {Abel L. Peirson and E. Meltem Tolunay},
year = {2018},
month = {June},
title = {Dank Learning: Generating Memes Using Deep Neural Networks},
note = {\\Accessed Jan 7 2022 [Online]}
}

@misc{tesseract_article,
author = {Anthony Kay},
year = {2007},
month = {July},
title = {Tesseract: an Open-Source Optical Character Recognition Engine},
note = {\\Accessed Jan 8 2022 [Online]}
}

@misc{pillow_about,
author = {Alex Clark and Contributors},
year = {2022},
title = {From Pillow Documentation},
note = {\\Accessed Jan 8 2022 [Online]}
}

@misc{eel_github,
author = {Chris Knott},
year = {2021},
title = {Eel GitHub},
note = {\\Accessed Jan 10 2022 [Online]}
}

@article{VALUEVA2020232,
title = {Application of the residue number system to reduce hardware costs of the convolutional neural network implementation},
journal = {Mathematics and Computers in Simulation},
volume = {177},
pages = {232-243},
year = {2020},
issn = {0378-4754},
doi = {https://doi.org/10.1016/j.matcom.2020.04.031},
url = {https://www.sciencedirect.com/science/article/pii/S0378475420301580},
author = {M.V. Valueva and N.N. Nagornov and P.A. Lyakhov and G.V. Valuev and N.I. Chervyakov},
keywords = {Image processing, Convolutional neural networks, Residue number system, Quantization noise, Field-programmable gate array (FPGA).},
abstract = {Convolutional neural networks are a promising tool for solving the problem of pattern recognition. Most well-known convolutional neural networks implementations require a significant amount of memory to store weights in the process of learning and working. We propose a convolutional neural network architecture in which the neural network is divided into hardware and software parts to increase performance and reduce the cost of implementation resources. We also propose to use the residue number system (RNS) in the hardware part to implement the convolutional layer of the neural network. Software simulations using Matlab 2018b showed that convolutional neural network with a minimum number of layers can be quickly and successfully trained. The hardware implementation of the convolution layer shows that the use of RNS allows to reduce the hardware costs on 7.86%–37.78% compared to the two’s complement implementation. The use of the proposed heterogeneous implementation reduces the average time of image recognition by 41.17%.}
}

@misc{neocognitron,
title = {Neocognitron: A Self-organizing Neural Network Model
for a Mechanism of Pattern Recognition
Unaffected by Shift in Position},
author = {Kunihiko Fukushima},
year = {1980},
journal = {Biological Cybernetics},
note = {\\Accessed Jan 11 2022 [Online]}
}

@misc{image_classification_article,
title = {Multi-column Deep Neural Networks for Image Classification},
author = {Dan Ciresan and Ueli Meier and Jurgen Schmidhuber},
year = {2012},
note = {\\Accessed Jan 11 2022 [Online]}
}

@misc{rnn_article,
title = {State-of-the-art in artificial neural network applications},
author = {Oludare Isaac Abiodun and Aman Jantan and Abiodun Esther Omolara and Kemi Victoria Dada and Nachaat AbdElatif Mohamed and Humaira Arshad},
year = {2018},
note = {\\Accessed Jan 11 2022 [Online]}
}

@article{10.1162/neco.1997.9.8.1735,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    title = "{Long Short-Term Memory}",
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    abstract = "{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}

@misc{ltsm_article,
title = {Sequence to Sequence Learning with Neural Networks},
author = {Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
year = {2014},
note = {\\Accessed Jan 12 2022 [Online]}
}

@misc{vanishing_weight,
title = {The Challenge of Vanishing/Exploding Gradients in Deep Neural Networks},
author = {Yash Bohra},
year = {2021},
note = {\\Accessed Jan 12 2022 [Online]}
}
